* comparison benchmarks
Those benchmarks are created to measure overhead of UMF, and showcase performance "gains"
There is <pool configs> X <Sizes> X <patterns> X <Threads> combinations.
** 1 set
*** Pools configs
- disjoint_pool<os_provider>
- jemalloc_pool<os_provider>
- scalable_pool<os_provider>
- disjoint_pool<coarse<os_provider>>
- jemalloc_pool<coarse<os_provider>>
- scalable_pool<coarse<os_provider>>
- plain glibc malloc 
- plain jemalloc
- plain scalablemalloc
- proxy lib (just run umf-benchmark --benchmark_filter="glibc")  
*** Sizes
- page alloc
- Variable random size <8 bytes, 64k>
*** Alloc pattern
- (maybe) 1000 alloc and free per loop
- preallocate 10000 allocs and then one alloc and free in the loop; 
*** Threads
- single threded
- multi threded (not sure how many, but a higher is better)
*** Charts
- One Bar chart for each config from set Sizes X Alloc Patterns X Threads.
- Each bar chart has all Pools configs on X axis, and performance metric on Y axis. 
** 2 set
*** Pools configs
- L0 provider with disjoint_pool
- Cuda provider with disjoint_pool
- plain L0
- plain cuda
*** Sizes
- tbd
***  Alloc pattern
- (maybe)1000 alloc and free per loop
- preallocate 10000 allocs and then 1000 alloc and free per loop
*** Threads
- single threded
- multi threded (probably 2-4 enough to check if we can scale on provider level)
*** Details
compare on single chart provider vs plain memory. In case of l0 we can consider testing different memory types
but from umf perspective, it should not matter.
*** Charts
- One Bar chart for each config from set Sizes X Alloc Patterns X Threads.
- So single bar chart has all pool configs on X axis.

  
* performance tracking benchmarks
This means as line chart, with performance metric on Y asix and commit ID(or date) on X axsis. 
When compassion will be ready, we will select some data points, and use them to create this kind of charts.

* IPC
For real IPC benchmark we need separate pool, but this will wait. We have to port existing ipcget benchmark form ubench before removal.


* Other Benchmarks
- proxy lib trashold
- diffrent os provider configurations (especialy those strange one which requires multiple mbinds)
- windows?
- realloc and calloc (propably realoc is top piority from this list)
  
* CTL
 - when CTL will be ready we can use it to bechmark fragmentation measured as
   <memory allocated from provider> vs <memory allocated from pool>
 - probably CTL might add extra test points (different allocator configurations), but this is TBD
